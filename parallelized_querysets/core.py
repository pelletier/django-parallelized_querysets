"""
Parallelized QuerySets.

This modules aims to handle large querysets by spreading their execution on
multiple cores and keeping the memory usage low.

First, some tasks are created. They are actually the representation of a query
set, limited to a few rows (specified by pks). Hence the whole set of tasks is
equivalent from a result point of view to the original querysets. (actually no:
the order of the results are *not* respected).

Then we create a few processes (ideally one per core) to handle the tasks.

When they are done working with the tasks, the master process aggregate the
results in one final list, and returns it.


Example:

    This will get all the Foo objects using 2 processes.

    >>> from myapp.models import Foo
    >>> parallelized_multiple_querysets([Foo.objects.all()], 2)
    [<Foo...>, ... <Foo...>]
"""
import gc
import time
import pickle
import datetime
from math import ceil
from sys import stdout
from Queue import Empty as Queue_Empty
from multiprocessing import Process, Queue, Array, cpu_count, current_process

from django.db import connections
from django.db.models.loading import get_model


class CustomProcess(Process):

    def __init__(self, *args, **kwargs):
        self.data = {}
        super(CustomProcess, self).__init__(*args, **kwargs)


def deconstruct_not_evaluated_queryset(qs):
    """
    Takes an queryset that has *never* been evaluated (even partially) and
    produces a tuple precise enough to reconstruct the QuerySet again.
    """
    model = qs.model
    app = model._meta.app_label
    name = str(model._meta).split('.')[-1]
    struct = {
        'query': qs.query,
        'db': qs._db
    }
    query = pickle.dumps(struct)

    return (app, name, query)


def construct_queryset(tup):
    """
    Takes a tuple generated by `deconstruct_not_evaluated_queryset` and build
    the representated QuerySet object.
    """
    app, name, query = tup
    model = get_model(app, name)
    struct = pickle.loads(query)
    qs = model.objects.using(struct['db']).all()
    qs.query = struct['query']
    return qs



def worker(uid, shared_states, in_queue, out_queue, msg_queue, function, init_hook, end_hook):
    """
    Used in a process.
    Takes some tasks from `in_queue` (those are querysets representations + pk
    offsets), process each row in the queryset's result with `function` and push
    the result to `out_queue` (along with a notification in `msg_queue`).
    """
    chunk_size = 500

    if not init_hook == None:
        init_hook(current_process())

    while True:
        try:
            # If the queue is empty just get out and die.
            qr_repr, start_pk, size = in_queue.get(timeout=1)
        except Queue_Empty:
            break

        # Slice the queryset to avoid loading to many items in memory.
        queryset = construct_queryset(qr_repr)
        last_pk = start_pk - size # We are filtering in descending order.
        delta = start_pk - last_pk
        res = []

        for i in xrange(int(ceil(delta/float(chunk_size)))):
            max_pk = start_pk-(i+1)*chunk_size
            if max_pk < last_pk:
                max_pk = last_pk
            min_pk = start_pk-i*chunk_size

            for row in queryset.filter(pk__gt=max_pk, pk__lte=min_pk):
                if function == None:
                    result = row
                else:
                    result = function(current_process(), row)
                # Return None in your function if you don't want to waste space.
                if result != None:
                    res.append(result)
            msg_queue.put(min_pk - max_pk)
            gc.collect()
        out_queue.put(res)

    if not end_hook == None:
        res = end_hook(current_process())
        if not res == None:
            out_queue.put([res])

    # We are out, mark us as done is the shared states.
    shared_states[uid] = 1


def writer(msg_queue, total):
    """
    Writer is used in a process to handle the progress display.

    total is the number of rows the workers are supposed to process.
    `msg_queue` is a queue where a message comes each time a row is processed.

    The current version refreshes every two seconds and display:
        - The amount of time since the process started.
        - A percentage indicating the amount of processed rows.
        - The number of requests per second.
    """
    last_output = ""
    buff = ""

    percent = 0
    reqs = 0
    current = 0

    start_time = time.time()
    last_time = time.time()

    for i in iter(msg_queue.get, 'STOP'):
        reqs += i
        current += i
        now = time.time()
        delta = now - last_time

        if delta > 2:
            speed = reqs / (now - last_time)
            last_time = now
            reqs = 0

            percent = current * 100.0 /total

            elapsed = now - start_time
            el_str = str(datetime.timedelta(seconds=elapsed))

            buff = "%s req/s %s%% elapsed time %s" % (speed, percent, el_str)

            stdout.write('\b'*len(last_output))
            stdout.write(buff)
            stdout.flush()
            last_output = buff


def parallelized_multiple_querysets(querysets, processes=None, function=None,
                                    init_hook=None, end_hook=None):
    """
    Create subtasks to process `querysets`. Fire `processes` processes and
    compute the results of `function` on each row of all querysets. The results
    are aggregated in a single list.
    """

    # Default to the number of available CPUs.
    if processes == None:
        processes = cpu_count()

    # To send tasks to the subprocesses.
    tasks_queue = Queue()
    # To gather the results produced by the subprocesses.
    results_queue = Queue()
    # To communicate to the writer to provide feedback to the user.
    msg_queue = Queue()
    # Each process has a cell to indicate its status (done or working).
    shared_states = Array('b', processes)

    total = 0

    # Split the querysets into chunks.
    # Feed them to the queue.
    for queryset in querysets:
        # Use the id of the first one as an approximation of the total number of
        # elements (way faster than COUNT).
        # Maybe try to use an approximate count on PG?
        try:
            total = queryset.order_by('-pk')[0].pk
        except IndexError:
            # There is nothing to process in this queryset.
            continue
        chunk_size = total / processes
        ordered_query_set = queryset.order_by('-pk')

        # Create a string representation of the query in order to pass them to
        # the workers (QuerySets are not pickleable).
        qs_repr = deconstruct_not_evaluated_queryset(ordered_query_set)

        # Create the tasks.
        for i in xrange(processes):
            size = chunk_size
            if i == processes - 1:
                size = total - i*chunk_size
            tasks_queue.put([qs_repr, total-i*chunk_size, size])


    if total == 0:
        return []

    # This is a hack.
    # We close the connections now (before forking). So, when any of the
    # subprocess will need a connection to a database, it will need to create
    # it, but this time it will be in its own context.
    # Obviously we can't share the same connection (i.e. UNIX socket) accross the
    # processes.
    for connection in connections.all():
        connection.close()


    # Create processes.
    for i in range(processes):
        process_args = (i,             # Process number (id).
                        shared_states, # Array representing the processes states.
                        tasks_queue,   # Input queue.
                        results_queue, # Output queue.
                        msg_queue,     # Messages (information) queue.
                        function,      # Processing function.
                        init_hook,     # Called when the Process starts
                        end_hook)      # Called when the Process ends
        CustomProcess(target=worker, args=process_args).start()

    # Create the writer process.
    Process(target=writer, args=(msg_queue, total)).start()

    # This is the expected end state (all processes are done).
    expected = [1]*processes

    # The final output will be aggregated here.
    aggregate = []

    # Wait for them to finish
    while True:
        try:
            # Aggregate results as they are available.
            # Maybe just build an iterator?
            aggregate += results_queue.get_nowait()
        except Queue_Empty:
            # If the queue is empty and we are not in the expected state, we
            # just keep listening (we consumed results faster than they are
            # produced, yeah!).
            if shared_states[0:] != expected:
                time.sleep(0.01)
            # Otherwise we are done.
            else:
                break

    # Ask the writer to shut down.
    msg_queue.put('STOP')

    # Return the result.
    return aggregate
